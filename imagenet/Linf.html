<table id="imagenet_leaderboard_Linf" class="datatable" style="width: 100%">
    <thead>
    <tr>
        <th class="rank">Rank</th>
        <th class="method">Method</th>
        <th class="ca">
            Standard<br/>
            accuracy
        </th>
        
        <th class="aa">
            AutoAttack<br/>
            robust<br/>
            accuracy
        </th>
        <th class="aa-ext">
            Best known<br/>
            robust<br/>
            accuracy
        </th>
        <th class="aa-ext">
            AA eval.<br/>
            potentially<br/>
            unreliable
        </th>
        
        
        
        <th class="extra-data">Extra <br/>data</th>
        <th class="arch">Architecture</th>
        <th class="venue">Venue</th>
    </tr>
    </thead>
    <tbody>
    
    <tr>
        <td class="ranktd">1</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2406.05927" target="_blank">MeanSparse: Post-Training Robustness Enhancement Through Mean-Centered Feature Sparsification</a>
            
            <br>
            <span class="td-footer">
                It adds the MeanSparse operator to the adversarially trained models.
            </span>
            
        </td>
        <td class="catd">77.96%</td>
        <td class="aatd">59.64%</td>
        <td class="aa-extd">59.64%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">MeanSparse ConvNeXt-L</td>
        <td class="venuetd">arXiv, Jun 2024</td>
    </tr>
    
    <tr>
        <td class="ranktd">2</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2302.14301" target="_blank">A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking</a>
            
        </td>
        <td class="catd">78.92%</td>
        <td class="aatd">59.56%</td>
        <td class="aa-extd">59.56%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">Swin-L</td>
        <td class="venuetd">arXiv, Feb 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">3</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2402.02263" target="_blank">MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers</a>
            
            <br>
            <span class="td-footer">
                It uses an ensemble of networks. The accurate base classifier was pre-trained on ImageNet-21k. 58.50% robust accuracy is due to the original evaluation (Adaptive AutoAttack)
            </span>
            
        </td>
        <td class="catd">81.48%</td>
        <td class="aatd">58.62%</td>
        <td class="aa-extd">58.50%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#9745;</td>
        <td class="archtd">ConvNeXtV2-L + Swin-L</td>
        <td class="venuetd">arXiv, Feb 2024</td>
    </tr>
    
    <tr>
        <td class="ranktd">4</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2302.14301" target="_blank">A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking</a>
            
        </td>
        <td class="catd">78.02%</td>
        <td class="aatd">58.48%</td>
        <td class="aa-extd">58.48%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ConvNeXt-L</td>
        <td class="venuetd">arXiv, Feb 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">5</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2303.01870" target="_blank">Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</a>
            
        </td>
        <td class="catd">77.00%</td>
        <td class="aatd">57.70%</td>
        <td class="aa-extd">57.70%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ConvNeXt-L + ConvStem</td>
        <td class="venuetd">NeurIPS 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">6</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2302.14301" target="_blank">A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking</a>
            
        </td>
        <td class="catd">76.16%</td>
        <td class="aatd">56.16%</td>
        <td class="aa-extd">56.16%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">Swin-B</td>
        <td class="venuetd">arXiv, Feb 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">7</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2303.01870" target="_blank">Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</a>
            
        </td>
        <td class="catd">75.90%</td>
        <td class="aatd">56.14%</td>
        <td class="aa-extd">56.14%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ConvNeXt-B + ConvStem</td>
        <td class="venuetd">NeurIPS 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">8</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2302.14301" target="_blank">A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking</a>
            
        </td>
        <td class="catd">76.02%</td>
        <td class="aatd">55.82%</td>
        <td class="aa-extd">55.82%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ConvNeXt-B</td>
        <td class="venuetd">arXiv, Feb 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">9</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2303.01870" target="_blank">Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</a>
            
        </td>
        <td class="catd">76.30%</td>
        <td class="aatd">54.66%</td>
        <td class="aa-extd">54.66%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ViT-B + ConvStem</td>
        <td class="venuetd">NeurIPS 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">10</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2303.01870" target="_blank">Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</a>
            
        </td>
        <td class="catd">74.10%</td>
        <td class="aatd">52.42%</td>
        <td class="aa-extd">52.42%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ConvNeXt-S + ConvStem</td>
        <td class="venuetd">NeurIPS 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">11</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2303.01870" target="_blank">Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</a>
            
        </td>
        <td class="catd">72.72%</td>
        <td class="aatd">49.46%</td>
        <td class="aa-extd">49.46%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ConvNeXt-T + ConvStem</td>
        <td class="venuetd">NeurIPS 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">12</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2308.16258" target="_blank">Robust Principles: Architectural Design Principles for Adversarially Robust CNNs</a>
            
        </td>
        <td class="catd">73.44%</td>
        <td class="aatd">48.94%</td>
        <td class="aa-extd">48.94%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">RaWideResNet-101-2</td>
        <td class="venuetd">BMVC 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">13</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2303.01870" target="_blank">Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</a>
            
        </td>
        <td class="catd">72.56%</td>
        <td class="aatd">48.08%</td>
        <td class="aa-extd">48.08%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ViT-S + ConvStem</td>
        <td class="venuetd">NeurIPS 2023</td>
    </tr>
    
    <tr>
        <td class="ranktd">14</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2209.07399" target="_blank">A Light Recipe to Train Robust Vision Transformers</a>
            
        </td>
        <td class="catd">73.76%</td>
        <td class="aatd">47.60%</td>
        <td class="aa-extd">47.60%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">XCiT-L12</td>
        <td class="venuetd">arXiv, Sep 2022</td>
    </tr>
    
    <tr>
        <td class="ranktd">15</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2209.07399" target="_blank">A Light Recipe to Train Robust Vision Transformers</a>
            
        </td>
        <td class="catd">74.04%</td>
        <td class="aatd">45.24%</td>
        <td class="aa-extd">45.24%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">XCiT-M12</td>
        <td class="venuetd">arXiv, Sep 2022</td>
    </tr>
    
    <tr>
        <td class="ranktd">16</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2209.07399" target="_blank">A Light Recipe to Train Robust Vision Transformers</a>
            
        </td>
        <td class="catd">72.34%</td>
        <td class="aatd">41.78%</td>
        <td class="aa-extd">41.78%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">XCiT-S12</td>
        <td class="venuetd">arXiv, Sep 2022</td>
    </tr>
    
    <tr>
        <td class="ranktd">17</td>
        <td class="methoddt">
            <a href="https://doi.org/10.1016/j.patcog.2024.110394" target="_blank">Data filtering for efficient adversarial training</a>
            
            <br>
            <span class="td-footer">
                
            </span>
            
        </td>
        <td class="catd">68.76%</td>
        <td class="aatd">40.60%</td>
        <td class="aa-extd">40.60%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">WideResNet-50-2</td>
        <td class="venuetd">Pattern Recognition 2024</td>
    </tr>
    
    <tr>
        <td class="ranktd">18</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2210.07540" target="_blank">When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture</a>
            
        </td>
        <td class="catd">74.66%</td>
        <td class="aatd">38.30%</td>
        <td class="aa-extd">38.30%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">Swin-B</td>
        <td class="venuetd">NeurIPS 2022</td>
    </tr>
    
    <tr>
        <td class="ranktd">19</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2007.08489" target="_blank">Do Adversarially Robust ImageNet Models Transfer Better?</a>
            
        </td>
        <td class="catd">68.46%</td>
        <td class="aatd">38.14%</td>
        <td class="aa-extd">38.14%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">WideResNet-50-2</td>
        <td class="venuetd">NeurIPS 2020</td>
    </tr>
    
    <tr>
        <td class="ranktd">20</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2007.08489" target="_blank">Do Adversarially Robust ImageNet Models Transfer Better?</a>
            
        </td>
        <td class="catd">64.02%</td>
        <td class="aatd">34.96%</td>
        <td class="aa-extd">34.96%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">NeurIPS 2020</td>
    </tr>
    
    <tr>
        <td class="ranktd">21</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2210.07540" target="_blank">When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture</a>
            
        </td>
        <td class="catd">68.38%</td>
        <td class="aatd">34.40%</td>
        <td class="aa-extd">34.40%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ViT-B</td>
        <td class="venuetd">NeurIPS 2022</td>
    </tr>
    
    <tr>
        <td class="ranktd">22</td>
        <td class="methoddt">
            <a href="https://github.com/MadryLab/robustness" target="_blank">Robustness library</a>
            
        </td>
        <td class="catd">62.56%</td>
        <td class="aatd">29.22%</td>
        <td class="aa-extd">29.22%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">GitHub,<br>Oct 2019</td>
    </tr>
    
    <tr>
        <td class="ranktd">23</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2001.03994" target="_blank">Fast is better than free: Revisiting adversarial training</a>
            
            <br>
            <span class="td-footer">
                Focuses on fast adversarial training.
            </span>
            
        </td>
        <td class="catd">55.62%</td>
        <td class="aatd">26.24%</td>
        <td class="aa-extd">26.24%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">ICLR 2020</td>
    </tr>
    
    <tr>
        <td class="ranktd">24</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2007.08489" target="_blank">Do Adversarially Robust ImageNet Models Transfer Better?</a>
            
        </td>
        <td class="catd">52.92%</td>
        <td class="aatd">25.32%</td>
        <td class="aa-extd">25.32%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-18</td>
        <td class="venuetd">NeurIPS 2020</td>
    </tr>
    
    <tr>
        <td class="ranktd">25</td>
        <td class="methoddt">
            <a href="https://github.com/RobustBench/robustbench/" target="_blank">Standardly trained model</a>
            
        </td>
        <td class="catd">76.52%</td>
        <td class="aatd">0.0%</td>
        <td class="aa-extd">0.0%</td>
        <td class="flagsd"><div class="flagsd-emoji">&#215;</div></td>
        
        
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">N/A</td>
    </tr>
    
    </tbody>
</table>
<script>
    $(document).ready(function () {
        $("#imagenet_leaderboard_Linf").DataTable({
            lengthMenu: [15, 25, 50, 75, 100],
            "drawCallback": function (settings) {
                MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
            },
            language: {
                searchPlaceholder: "Papers, architectures, venues"
            },
            
            columnDefs: [
                { width: "15%", targets: 4 },
                { width: "15%", targets: 5 }
            ]
            
        });
    });
</script>
