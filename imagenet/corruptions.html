<table id="imagenet_leaderboard_corruptions" class="datatable" style="width: 100%">
    <thead>
    <tr>
        <th class="rank">Rank</th>
        <th class="method">Method</th>
        <th class="ca">
            Standard<br/>
            accuracy
        </th>


        <th class="aa">
            Robust<br/>
            accuracy<br/>
            (IN-C)
        </th>


        <th class="aa_3d">
            Robust<br/>
            accuracy<br/>
            (IN-3DCC)
        </th>

        <th class="extra-data">Extra <br/>data</th>
        <th class="arch">Architecture</th>
        <th class="venue">Venue</th>
    </tr>
    </thead>
    <tbody>

    <tr>
        <td class="ranktd">1</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2204.12143" target="_blank">Deeper Insights into the Robustness of ViTs towards Common Corruptions</a>

        </td>
        <td class="catd">81.38%</td>
        <td class="aatd">67.55%</td>
        <td class="aatd_3d">65.9%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">DeiT Base</td>
        <td class="venuetd">arXiv, Apr 2022</td>
    </tr>

    <tr>
        <td class="ranktd">2</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2204.12143" target="_blank">Deeper Insights into the Robustness of ViTs towards Common Corruptions</a>

        </td>
        <td class="catd">79.76%</td>
        <td class="aatd">62.91%</td>
        <td class="aatd_3d">62.64%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">DeiT Small</td>
        <td class="venuetd">arXiv, Apr 2022</td>
    </tr>

    <tr>
        <td class="ranktd">3</td>
        <td class="methoddt">
            <a href="https://arxiv.org/pdf/2202.01263.pdf" target="_blank">NoisyMix: Boosting Robustness by Combining Data Augmentations, Stability Training, and Noise Injections</a>

            <br>
            <span class="td-footer">
                Better tuned model.
            </span>

        </td>
        <td class="catd">76.9%</td>
        <td class="aatd">53.28%</td>
        <td class="aatd_3d">56.1%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">arXiv, Feb 2022</td>
    </tr>

    <tr>
        <td class="ranktd">4</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2006.16241" target="_blank">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</a>

        </td>
        <td class="catd">76.86%</td>
        <td class="aatd">52.9%</td>
        <td class="aatd_3d">56.51%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">ICCV 2021</td>
    </tr>

    <tr>
        <td class="ranktd">5</td>
        <td class="methoddt">
            <a href="https://arxiv.org/pdf/2202.01263.pdf" target="_blank">NoisyMix: Boosting Robustness by Combining Data Augmentations, Stability Training, and Noise Injections</a>

            <br>
            <span class="td-footer">
                Basic model.
            </span>

        </td>
        <td class="catd">76.98%</td>
        <td class="aatd">52.47%</td>
        <td class="aatd_3d">55.79%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">arXiv, Feb 2022</td>
    </tr>

    <tr>
        <td class="ranktd">6</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/1912.02781" target="_blank">AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty</a>

        </td>
        <td class="catd">77.34%</td>
        <td class="aatd">49.33%</td>
        <td class="aatd_3d">53.48%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">ICLR 2020</td>
    </tr>

    <tr>
        <td class="ranktd">7</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/1811.12231" target="_blank">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</a>

            <br>
            <span class="td-footer">
                Model B: trained on Stylized ImageNet and standard ImageNet.
            </span>

        </td>
        <td class="catd">74.98%</td>
        <td class="aatd">45.76%</td>
        <td class="aatd_3d">50.03%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">ICLR 2019</td>
    </tr>

    <tr>
        <td class="ranktd">8</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/1811.12231" target="_blank">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</a>

            <br>
            <span class="td-footer">
                Model C: trained on Stylized ImageNet and standard ImageNet, then fine-tuned on standard ImageNet.
            </span>

        </td>
        <td class="catd">77.56%</td>
        <td class="aatd">42.0%</td>
        <td class="aatd_3d">49.03%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">ICLR 2019</td>
    </tr>

    <tr>
        <td class="ranktd">9</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/1811.12231" target="_blank">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</a>

            <br>
            <span class="td-footer">
                Model A: trained on Stylized ImageNet.
            </span>

        </td>
        <td class="catd">60.08%</td>
        <td class="aatd">39.92%</td>
        <td class="aatd_3d">40.46%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">ICLR 2019</td>
    </tr>

    <tr>
        <td class="ranktd">10</td>
        <td class="methoddt">
            <a href="https://github.com/RobustBench/robustbench/" target="_blank">Standardly trained model</a>

        </td>
        <td class="catd">76.72%</td>
        <td class="aatd">39.48%</td>
        <td class="aatd_3d">47.12%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">ResNet-50</td>
        <td class="venuetd">N/A</td>
    </tr>

    <tr>
        <td class="ranktd">11</td>
        <td class="methoddt">
            <a href="https://arxiv.org/abs/2007.08489" target="_blank">Do Adversarially Robust ImageNet Models Transfer Better?</a>

        </td>
        <td class="catd">68.64%</td>
        <td class="aatd">36.09%</td>
        <td class="aatd_3d">41.86%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">WideResNet-50-2</td>
        <td class="venuetd">NeurIPS 2020</td>
    </tr>

    <tr>
        <td class="ranktd">12</td>
        <td class="methoddt">
            <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" target="_blank">ImageNet Classification with Deep Convolutional Neural Networks</a>

        </td>
        <td class="catd">56.24%</td>
        <td class="aatd">21.12%</td>
        <td class="aatd_3d">28.47%</td>
        <td class="datatd">&#215;</td>
        <td class="archtd">AlexNet</td>
        <td class="venuetd">NeurIPS 2012</td>
    </tr>

    </tbody>
</table>
<script>
    $(document).ready(function () {
        $("#imagenet_leaderboard_corruptions").DataTable({
            lengthMenu: [15, 25, 50, 75, 100],
            "drawCallback": function (settings) {
                MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
            },
            language: {
                searchPlaceholder: "Papers, architectures, venues"
            },

        });
    });
</script>